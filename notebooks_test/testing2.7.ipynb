{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "import ast\n",
    "from ast import AST\n",
    "import tokenize\n",
    "import token\n",
    "from numbers import Number\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWith(start, tk):\n",
    "    if(start):\n",
    "        return start['line'] < tk.start[0] or (start['line'] == tk.start[0] and start['ch'] <= tk.start[1])\n",
    "    return False\n",
    "\n",
    "def getStart(node):\n",
    "    if(hasattr(node, 'lineno')):\n",
    "        return {'line' : node.lineno, 'ch': node.col_offset }\n",
    "    else: # try to get to the first subnode that has a lineno\n",
    "        child = next(ast.iter_child_nodes(node), None)\n",
    "        loc = getStart(child) if child else None\n",
    "        return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatToken(tk):\n",
    "    ban = [token.NEWLINE, token.DEDENT, token.INDENT, token.OP]\n",
    "    range = {'start': {'line': tk.start[0], 'ch': tk.start[1]}, 'end': {'line': tk.end[0], 'ch': tk.end[1]}}\n",
    "    if (tk.type in ban) or token.tok_name[tk.type] == 'NL':\n",
    "        print(tk.type, ban)\n",
    "        range['syntok'] = tk.string\n",
    "        return range\n",
    "    range['literal'] = tk.string\n",
    "    range['type'] = token.tok_name[tk.type]\n",
    "    return range\n",
    "\n",
    "def formatTokenList(tk_list):\n",
    "    formatted = []\n",
    "    for tk in tk_list:\n",
    "        fm = formatToken(tk)\n",
    "        if fm:\n",
    "            formatted.append(fm)\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitBeforeTokens(content, nodey, before_tokens):\n",
    "    prevNodey = content[-1] if content != [] else None\n",
    "    middle = []\n",
    "    nodeyMatch = []\n",
    "    for tk in before_tokens:\n",
    "        #print('SPLIT prevNodey is', prevNodey, 'token is', tk, 'nodey is', nodey)\n",
    "        if(nodey and tk['start']['line'] == nodey['start']['line']):\n",
    "            nodeyMatch.append(tk)\n",
    "        elif(prevNodey and tk['start']['line'] == prevNodey['start']['line']):\n",
    "            content.append(tk)\n",
    "        else:\n",
    "            middle.append(tk)\n",
    "    if nodeyMatch != []:\n",
    "        print(\"DOING SUMthing with nodeyMatch, \", nodeyMatch)\n",
    "        nodey['content'] = nodeyMatch + (nodey['content'] or [])\n",
    "    print(middle)\n",
    "    return content, middle, nodey\n",
    "\n",
    "\n",
    "def splitAfterTokens(prevStart, nodeStart, after_tokens):\n",
    "    middle = []\n",
    "    nodeyMatch = []\n",
    "    for tok in after_tokens:\n",
    "        if nodeStart and tok.start[0] == nodeStart['line'] and ((not prevStart) or tok.start[0] != prevStart['line']):\n",
    "            nodeyMatch.append(tok)\n",
    "        else:\n",
    "            middle.append(tok)\n",
    "    return middle, nodeyMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTokens_before(node, tokenList):\n",
    "    chunkList = []\n",
    "    before = []\n",
    "    child = next(ast.iter_child_nodes(node), None)\n",
    "    if child:\n",
    "        start = getStart(child)\n",
    "        if(start):\n",
    "            for chunk in tokenList:\n",
    "                if(startsWith(start, chunk)): #start of child\n",
    "                    chunkList.append(chunk)\n",
    "                else:\n",
    "                    before.append(chunk)\n",
    "            before_formatted = formatTokenList(before)\n",
    "            return before_formatted, chunkList\n",
    "\n",
    "    return [], tokenList\n",
    "\n",
    "\n",
    "\n",
    "def processTokens_middle(node, tokenList):\n",
    "    children = ast.iter_child_nodes(node)\n",
    "    child1 = next(children, None)\n",
    "    content = []\n",
    "    #print(\"child is\", list(ast.iter_child_nodes(node)))\n",
    "    \n",
    "    if child1:\n",
    "        chunkList = []\n",
    "        child2 = next(children, None)\n",
    "        child2_start = None\n",
    "        while(child2 and not child2_start):\n",
    "            child2_start = getStart(child2)\n",
    "            if(not child2_start): \n",
    "                child2 = next(children, None)\n",
    "        if(child2):\n",
    "            child1_start = getStart(child1)\n",
    "            #print(\"child #1\", child1, child1_start, \"child #2\", child2, child2_start, \"\\n\")\n",
    "            for chunk in tokenList: \n",
    "                if(child2 and startsWith(child2_start, chunk)): #start of child 2\n",
    "                    #first, give all the tokens collected so far to child1. child2 starts with what remains\n",
    "                    before_tokens, child_nodey, after_tokens = zipTokensAST(chunkList, child1)\n",
    "                    content, before_tokens, child_nodey = splitBeforeTokens(content, child_nodey, before_tokens)\n",
    "                    #print(\"ADDING BEFORE\", before_tokens)\n",
    "                    content += before_tokens\n",
    "                    if child_nodey: content.append(child_nodey)\n",
    "                    chunkList = []\n",
    "                    if(after_tokens != []):\n",
    "                        after_tokens, chunkList = splitAfterTokens(child1_start, child2_start, after_tokens)\n",
    "                        content += formatTokenList(after_tokens)\n",
    "                    child1_start = child2_start\n",
    "                    child1 = child2\n",
    "                    child2 = next(children, None)\n",
    "                    child2_start = getStart(child2) if child2 else None\n",
    "                    #print(\"child #1 is now:\", child1, child1_start, \"child #2\", child2, child2_start, \"\\n\", content)\n",
    "        \n",
    "                chunkList.append(chunk)\n",
    "        else: \n",
    "            chunkList = tokenList\n",
    "     \n",
    "        before_tokens, child_nodey, after_tokens = zipTokensAST(chunkList, child1)\n",
    "        if(child_nodey):\n",
    "            content, before_tokens, child_nodey = splitBeforeTokens(content, child_nodey, before_tokens)\n",
    "        content += before_tokens\n",
    "        if child_nodey: content.append(child_nodey)\n",
    "        chunkList = after_tokens\n",
    "        return content, chunkList\n",
    "    else:\n",
    "        # no children, but eat what can\n",
    "        start = getStart(node)\n",
    "        content = []\n",
    "        if(start != None and tokenList != []):\n",
    "            if(tokenList[0].start[0] == start['line'] and tokenList[0].start[1] == start['ch']):\n",
    "                content += formatTokenList([tokenList.pop(0)])\n",
    "                #end = content[-1]\n",
    "        return content, tokenList\n",
    "        \n",
    "\n",
    "def zipTokensAST(tokens, node):\n",
    "    #print (\"\\nTreating\", type(node).__name__ )\n",
    "    \n",
    "    before_tokens, tokens = processTokens_before(node, tokens)\n",
    "    #if before_tokens !=[] : print(\"got before_tokens\", before_tokens)\n",
    "    \n",
    "    content, remainder = processTokens_middle(node, tokens)\n",
    "    #print(\"got content \", content)\n",
    "    if(content != []):\n",
    "        if(remainder != []):\n",
    "            print(content[-1])\n",
    "            remainder, chunkList = splitAfterTokens(None, content[-1]['start'], remainder)\n",
    "            content += formatTokenList(chunkList)\n",
    "        nodey = {'type': type(node).__name__, 'start': content[0]['start'], 'end': content[-1]['end'], 'content': content}\n",
    "        #print (\"\\nCompiling\", type(node).__name__ )\n",
    "        #print(\"\\nnode:\", nodey, \"\\nafter:\", remainder, \"\\n\\n\")\n",
    "    else:\n",
    "        nodey = None\n",
    "    return before_tokens, nodey, remainder\n",
    "\n",
    "\n",
    "\n",
    "def addBackSpaces(tokens):\n",
    "    prior = None\n",
    "    fixedList = []\n",
    "    for tok in tokens:\n",
    "        if(prior):\n",
    "            start = tok.start\n",
    "            #check for a gap. prior end and tok start should match\n",
    "            if(prior.end[0] == start[0]): # same line\n",
    "                if(prior.end[1] != start[1]): # different character\n",
    "                    space = start[1] - prior.end[1]\n",
    "                    fixedList.append(SpacerToken(token.OP, \" \"*space, [prior.end[0], prior.end[1]], [start[0], start[1]]))\n",
    "        fixedList.append(tok)\n",
    "        prior = tok\n",
    "    return fixedList\n",
    "\n",
    "\n",
    "class SpacerToken:\n",
    "    def __init__(self, ty, st, start, end):\n",
    "        self.type = ty\n",
    "        self.string = st\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "\n",
    "def main(text):\n",
    "    #first, check if the code is empty\n",
    "    if(text == \"\"): print(\"\")\n",
    "    # now check if this file is python 2.7 or python 3\n",
    "    # can't trust the user's current active kernel, because they may\n",
    "    # be on a python 3 kernel and then open a file written in python 2.7\n",
    "    parsePy3(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePy2(text):\n",
    "    tree = ast.parse(text)\n",
    "    split = text.split('\\n')\n",
    "    bytes = io.BytesIO(text.encode())\n",
    "    g = tokenize.tokenize(bytes.readline)\n",
    "    \n",
    "    tokens = list(g)\n",
    "    tokens = addBackSpaces(tokens)\n",
    "    #print(tokens)\n",
    "    tokens.pop(0) #get rid of encoding stuff\n",
    "    before_tokens, nodey, remainder = zipTokensAST(tokens, tree)\n",
    "    if nodey is None:\n",
    "        nodey = {'start': {'line': 0, 'ch': 0}, 'end': {'line': 0, 'ch': 0}, 'content': []}\n",
    "    nodey['content'] = before_tokens + nodey['content'] + formatTokenList(remainder)\n",
    "    nodey['start'] = nodey['content'][0]['start']\n",
    "    nodey['end'] = nodey['content'][-1]['end']\n",
    "    nodey['content'].pop() #remove end marker\n",
    "    print (json.dumps(nodey, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePy3(text):\n",
    "    tree = ast.parse(text)\n",
    "    split = text.split('\\n')\n",
    "    bytes = io.BytesIO(text.encode())\n",
    "    g = tokenize.tokenize(bytes.readline)\n",
    "    \n",
    "    tokens = list(g)\n",
    "    tokens = addBackSpaces(tokens)\n",
    "    #print(tokens)\n",
    "    tokens.pop(0) #get rid of encoding stuff\n",
    "    before_tokens, nodey, remainder = zipTokensAST(tokens, tree)\n",
    "    if nodey is None:\n",
    "        nodey = {'start': {'line': 0, 'ch': 0}, 'end': {'line': 0, 'ch': 0}, 'content': []}\n",
    "    nodey['content'] = before_tokens + nodey['content'] + formatTokenList(remainder)\n",
    "    nodey['start'] = nodey['content'][0]['start']\n",
    "    nodey['end'] = nodey['content'][-1]['end']\n",
    "    nodey['content'].pop() #remove end marker\n",
    "    if nodey.get('type', None) == None:\n",
    "        nodey['type'] = \"Module\" #to be consistent, the outer shell should be Module\n",
    "    print (json.dumps(nodey, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"Module\",\n",
      "  \"start\": {\n",
      "    \"line\": 1,\n",
      "    \"ch\": 1\n",
      "  },\n",
      "  \"end\": {\n",
      "    \"line\": 1,\n",
      "    \"ch\": 7\n",
      "  },\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"type\": \"Expr\",\n",
      "      \"start\": {\n",
      "        \"line\": 1,\n",
      "        \"ch\": 1\n",
      "      },\n",
      "      \"end\": {\n",
      "        \"line\": 1,\n",
      "        \"ch\": 7\n",
      "      },\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"List\",\n",
      "          \"start\": {\n",
      "            \"line\": 1,\n",
      "            \"ch\": 1\n",
      "          },\n",
      "          \"end\": {\n",
      "            \"line\": 1,\n",
      "            \"ch\": 7\n",
      "          },\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"start\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 0\n",
      "              },\n",
      "              \"end\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 1\n",
      "              },\n",
      "              \"syntok\": \"[\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"Num\",\n",
      "              \"start\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 1\n",
      "              },\n",
      "              \"end\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 3\n",
      "              },\n",
      "              \"content\": [\n",
      "                {\n",
      "                  \"start\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 1\n",
      "                  },\n",
      "                  \"end\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 2\n",
      "                  },\n",
      "                  \"literal\": \"1\",\n",
      "                  \"type\": \"NUMBER\"\n",
      "                },\n",
      "                {\n",
      "                  \"start\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 2\n",
      "                  },\n",
      "                  \"end\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 3\n",
      "                  },\n",
      "                  \"syntok\": \",\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"Num\",\n",
      "              \"start\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 3\n",
      "              },\n",
      "              \"end\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 5\n",
      "              },\n",
      "              \"content\": [\n",
      "                {\n",
      "                  \"start\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 3\n",
      "                  },\n",
      "                  \"end\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 4\n",
      "                  },\n",
      "                  \"literal\": \"2\",\n",
      "                  \"type\": \"NUMBER\"\n",
      "                },\n",
      "                {\n",
      "                  \"start\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 4\n",
      "                  },\n",
      "                  \"end\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 5\n",
      "                  },\n",
      "                  \"syntok\": \",\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"Num\",\n",
      "              \"start\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 5\n",
      "              },\n",
      "              \"end\": {\n",
      "                \"line\": 1,\n",
      "                \"ch\": 7\n",
      "              },\n",
      "              \"content\": [\n",
      "                {\n",
      "                  \"start\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 5\n",
      "                  },\n",
      "                  \"end\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 6\n",
      "                  },\n",
      "                  \"literal\": \"3\",\n",
      "                  \"type\": \"NUMBER\"\n",
      "                },\n",
      "                {\n",
      "                  \"start\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 6\n",
      "                  },\n",
      "                  \"end\": {\n",
      "                    \"line\": 1,\n",
      "                    \"ch\": 7\n",
      "                  },\n",
      "                  \"syntok\": \"]\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = \"[1,2,3]\"\n",
    "\n",
    "# to hurry up, reduce ast at this stage?\n",
    "# match parens [] {} () app\n",
    "\n",
    "main(text)\n",
    "#print(json.dumps(main(l, tree),  indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
