{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "import ast\n",
    "from ast import AST\n",
    "import tokenize\n",
    "import token\n",
    "from numbers import Number\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWith(start, tk):\n",
    "    if(start):\n",
    "        return start['line'] < tk.start[0] or (start['line'] == tk.start[0] and start['ch'] <= tk.start[1])\n",
    "    return False\n",
    "\n",
    "def getStart(node):\n",
    "    if(hasattr(node, 'lineno')):\n",
    "        return {'line' : node.lineno, 'ch': node.col_offset }\n",
    "    else: # try to get to the first subnode that has a lineno\n",
    "        child = next(ast.iter_child_nodes(node), None)\n",
    "        loc = getStart(child) if child else None\n",
    "        return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatToken(tk):\n",
    "    ban = [token.NEWLINE, token.DEDENT, token.INDENT, token.OP]\n",
    "    range = {'start': {'line': tk.start[0], 'ch': tk.start[1]}, 'end': {'line': tk.end[0], 'ch': tk.end[1]}}\n",
    "    if (tk.type in ban) or token.tok_name[tk.type] == 'NL':\n",
    "        range['syntok'] = tk.string\n",
    "        return range\n",
    "    range['literal'] = tk.string\n",
    "    range['type'] = token.tok_name[tk.type]\n",
    "    return range\n",
    "\n",
    "def formatTokenList(tk_list):\n",
    "    formatted = []\n",
    "    for tk in tk_list:\n",
    "        fm = formatToken(tk)\n",
    "        formatted.append(fm)\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitBeforeTokens(content, nodey, before_tokens):\n",
    "    prevNodey = content[-1] if content != [] else None\n",
    "    middle = []\n",
    "    nodeyMatch = []\n",
    "    for tk in before_tokens:\n",
    "        #print('prevNodey is', prevNodey, 'token is', tk, 'nodey is', nodey)\n",
    "        if(nodey and tk['start']['line'] == nodey['start']['line']):\n",
    "            nodeyMatch.append(tk)\n",
    "        elif(prevNodey and tk['start']['line'] == prevNodey['start']['line']):\n",
    "            content += tk\n",
    "        else:\n",
    "            middle += tk\n",
    "    if nodeyMatch != []:\n",
    "        nodey['content'] = nodeyMatch + (nodey['content'] or [])\n",
    "    return content, middle, nodey\n",
    "\n",
    "\n",
    "def splitAfterTokens(prevStart, nodeStart, after_tokens):\n",
    "    middle = []\n",
    "    nodeyMatch = []\n",
    "    for tok in after_tokens:\n",
    "        if nodeStart and tok.start[0] == nodeStart['line'] and ((not prevStart) or tok.start[0] != prevStart['line']):\n",
    "            nodeyMatch.append(tok)\n",
    "        else:\n",
    "            middle.append(tok)\n",
    "    return middle, nodeyMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTokens_before(node, tokenList):\n",
    "    chunkList = []\n",
    "    before = []\n",
    "    child = next(ast.iter_child_nodes(node), None)\n",
    "    if child:\n",
    "        start = getStart(child)\n",
    "        if(start):\n",
    "            for chunk in tokenList:\n",
    "                if(startsWith(start, chunk)): #start of child\n",
    "                    chunkList.append(chunk)\n",
    "                else:\n",
    "                    before.append(chunk)\n",
    "            before_formatted = formatTokenList(before)\n",
    "            return before_formatted, chunkList\n",
    "\n",
    "    return [], tokenList\n",
    "\n",
    "\n",
    "\n",
    "def processTokens_middle(node, tokenList):\n",
    "    children = ast.iter_child_nodes(node)\n",
    "    child1 = next(children, None)\n",
    "    content = []\n",
    "    \n",
    "    if child1:\n",
    "        chunkList = []\n",
    "        child2 = next(children, None)\n",
    "        child2_start = None\n",
    "        while(child2 and not child2_start):\n",
    "            child2_start = getStart(child2)\n",
    "            if(not child2_start): \n",
    "                child2 = next(children, None)\n",
    "        if(child2):\n",
    "            child1_start = getStart(child1)\n",
    "            #print(\"child #1\", child1, child1_start, \"child #2\", child2, child2_start, \"\\n\")\n",
    "            for chunk in tokenList: \n",
    "                if(child2 and startsWith(child2_start, chunk)): #start of child 2\n",
    "                    #first, give all the tokens collected so far to child1. child2 starts with what remains\n",
    "                    before_tokens, child_nodey, after_tokens = zipTokensAST(chunkList, child1)\n",
    "                    content, before_tokens, child_nodey = splitBeforeTokens(content, child_nodey, before_tokens)\n",
    "                    content += before_tokens\n",
    "                    if child_nodey: content.append(child_nodey)\n",
    "                    chunkList = []\n",
    "                    if(after_tokens != []):\n",
    "                        after_tokens, chunkList = splitAfterTokens(child1_start, child2_start, after_tokens)\n",
    "                        content += formatTokenList(after_tokens)\n",
    "                    child1_start = child2_start\n",
    "                    child1 = child2\n",
    "                    child2 = next(children, None)\n",
    "                    child2_start = getStart(child2) if child2 else None\n",
    "                    #print(\"child #1 is now:\", child1, child1_start, \"child #2\", child2, child2_start, \"\\n\", content)\n",
    "        \n",
    "                chunkList.append(chunk)\n",
    "        else: \n",
    "            chunkList = tokenList\n",
    "     \n",
    "        before_tokens, child_nodey, after_tokens = zipTokensAST(chunkList, child1)\n",
    "        content, before_tokens, child_nodey = splitBeforeTokens(content, child_nodey, before_tokens)\n",
    "        content += before_tokens\n",
    "        if child_nodey: content.append(child_nodey)\n",
    "        chunkList = after_tokens\n",
    "        return content, chunkList\n",
    "    else:\n",
    "        # no children, but eat what can\n",
    "        start = getStart(node)\n",
    "        #print(\"my start\", start)\n",
    "        content = []\n",
    "        if(start):\n",
    "            if(tokenList[0].start[0] == start['line'] and tokenList[0].start[1] == start['ch']):\n",
    "                content += formatTokenList([tokenList.pop(0)])\n",
    "                #end = content[-1]\n",
    "        return content, tokenList\n",
    "        \n",
    "\n",
    "def zipTokensAST(tokens, node):\n",
    "    print (\"\\nTreating\", type(node).__name__ )\n",
    "    \n",
    "    before_tokens, tokens = processTokens_before(node, tokens)\n",
    "    if before_tokens !=[] : print(\"got before_tokens\", before_tokens)\n",
    "    \n",
    "    content, remainder = processTokens_middle(node, tokens)\n",
    "    #print(\"got content \", content, remainder)\n",
    "    if(content != []):\n",
    "        if(remainder != []):\n",
    "            remainder, chunkList = splitAfterTokens(None, content[-1]['start'], remainder)\n",
    "            content += formatTokenList(chunkList)\n",
    "        nodey = {'type': type(node).__name__, 'start': content[0]['start'], 'end': content[-1]['end'], 'content': content}\n",
    "        print (\"\\nCompiling\", type(node).__name__ )\n",
    "        print(\"\\nnode:\", nodey, \"\\nafter:\", remainder, \"\\n\\n\")\n",
    "    else:\n",
    "        nodey = None\n",
    "    return before_tokens, nodey, remainder\n",
    "\n",
    "\n",
    "\n",
    "def addBackSpaces(tokens):\n",
    "    prior = None\n",
    "    fixedList = []\n",
    "    for tok in tokens:\n",
    "        if(prior):\n",
    "            start = tok.start\n",
    "            #check for a gap. prior end and tok start should match\n",
    "            if(prior.end[0] == start[0]): # same line\n",
    "                if(prior.end[1] != start[1]): # different character\n",
    "                    space = start[1] - prior.end[1]\n",
    "                    fixedList.append(SpacerToken(token.OP, \" \"*space, [prior.end[0], prior.end[1]], [start[0], start[1]]))\n",
    "        fixedList.append(tok)\n",
    "        prior = tok\n",
    "    return fixedList\n",
    "\n",
    "\n",
    "class SpacerToken:\n",
    "    def __init__(self, ty, st, start, end):\n",
    "        self.type = ty\n",
    "        self.string = st\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "\n",
    "def main(text):\n",
    "    tree = ast.parse(text)\n",
    "    split = text.split('\\n')\n",
    "    bytes = io.BytesIO(text.encode())\n",
    "    g = tokenize.tokenize(bytes.readline)\n",
    "    \n",
    "    tokens = list(g)\n",
    "    tokens = addBackSpaces(tokens)\n",
    "    #print(tokens)\n",
    "    tokens.pop(0) #get rid of encoding stuff\n",
    "    before_tokens, nodey, remainder = zipTokensAST(tokens, tree)\n",
    "    if nodey is None:\n",
    "        nodey = {'start': {'line': tk.start[0], 'ch': tk.start[1]}, 'end': {'line': tk.end[0], 'ch': tk.end[1]}}\n",
    "    nodey['content'] = before_tokens + nodey['content'] + formatTokenList(remainder)\n",
    "    nodey['start'] = nodey['content'][0]['start']\n",
    "    nodey['end'] = nodey['content'][-1]['end']\n",
    "    nodey['content'].pop() #remove end marker\n",
    "    print (json.dumps(nodey, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8d5cb1cb1e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"import os\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(json.dumps(main(l, tree),  indent=2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-114ffec0e18f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/tokenize.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(readline, tokeneater)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtokenize_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokeneater\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopTokenizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/tokenize.pyc\u001b[0m in \u001b[0;36mtokenize_loop\u001b[0;34m(readline, tokeneater)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokeneater\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mtokeneater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtoken_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUntokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "text = \"\"\"import os\"\"\"\n",
    "\n",
    "main(text)\n",
    "#print(json.dumps(main(l, tree),  indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
