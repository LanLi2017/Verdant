{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retrieval\n",
    "\n",
    "This guide will introduce techniques for organizing text data. It will show how to analyze a large corpus of text, extracting feature vectors for individual documents, in order to be able to retrieve documents with similar content.\n",
    "\n",
    "[scipy](https://www.scipy.org/) and [scikit-learn](scikit-learn.org) are required to run through this document, as well as a corpus of text documents. This code can be adapted to work with a set of documents you collect. For the purpose of this example, we will use the well-known Reuters-21578 dataset with 90 categories. To download this dataset, download it manually from [here](http://disi.unitn.it/moschitti/corpora.htm), or run `download.sh` in the `data` folder (to get all the other data for ml4a-guides as well), or just run:\n",
    "\n",
    "    wget http://disi.unitn.it/moschitti/corpora/Reuters21578-Apte-90Cat.tar.gz\n",
    "    tar -xzf Reuters21578-Apte-90Cat.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've downloaded and unzipped the dataset, take a look inside the folder. It is split into two folders, \"training\" and \"test\". Each of those contains 91 subfolders, corresponding to pre-labeled categories, which will be useful for us later when we want to try classifying the category of an unknown message. In this notebook, we are not worried about training a classifier, so we'll end up using both sets together.\n",
    "\n",
    "Let's note the location of the folder into a variable `data_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data/Reuters21578-Apte-90Cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open up a single message and look at the contents. This is the very first message in the training folder, inside of the \"acq\" folder, which is a category apparently containing news of corporate acquisitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE\n",
      "\n",
      "     COMMACK, N.Y., Feb 26 - Computer Terminal Systems Inc said\n",
      "it has completed the sale of 200,000 shares of its common\n",
      "stock, and warrants to acquire an additional one mln shares, to\n",
      "<Sedio N.V.> of Lugano, Switzerland for 50,000 dlrs.\n",
      "    The company said the warrants are exercisable for five\n",
      "years at a purchase price of .125 dlrs per share.\n",
      "    Computer Terminal said Sedio also has the right to buy\n",
      "additional shares and increase its total holdings up to 40 pct\n",
      "of the Computer Terminal's outstanding common stock under\n",
      "certain circumstances involving change of control at the\n",
      "company.\n",
      "    The company said if the conditions occur the warrants would\n",
      "be exercisable at a price equal to 75 pct of its common stock's\n",
      "market price at the time, not to exceed 1.50 dlrs per share.\n",
      "    Computer Terminal also said it sold the technolgy rights to\n",
      "its Dot Matrix impact technology, including any future\n",
      "improvements, to <Woodco Inc> of Houston, Tex. for 200,000\n",
      "dlrs. But, it said it would continue to be the exclusive\n",
      "worldwide licensee of the technology for Woodco.\n",
      "    The company said the moves were part of its reorganization\n",
      "plan and would help pay current operation costs and ensure\n",
      "product delivery.\n",
      "    Computer Terminal makes computer generated labels, forms,\n",
      "tags and ticket printers and terminals.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_path = os.path.join(data_dir, \"training\", \"acq\", \"0000005\")\n",
    "with open (post_path, \"r\") as p:\n",
    "    raw_text = p.read()\n",
    "    print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our collection contains over 15,000 articles with a lot of information. It would take way too long to get through all the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tin', 'l-cattle', 'fuel', 'meal-feed', 'heat', 'palm-oil', 'interest', 'sugar', 'gas', 'gnp', 'cotton-oil', 'dlr', 'soybean', 'coconut-oil', 'lin-oil', 'ipi', 'bop', 'cpi', 'strategic-metal', 'naphtha', 'jet', 'rice', 'cpu', 'trade', 'oilseed', 'nkr', 'groundnut', 'potato', 'sun-meal', 'lumber', 'dmk', 'silver', 'copper', 'cotton', 'soy-oil', 'rape-oil', 'rubber', 'income', 'copra-cake', 'earn', 'livestock', 'housing', 'money-fx', 'retail', 'palmkernel', 'rapeseed', 'tea', 'grain', 'gold', 'wpi', 'nzdlr', 'platinum', 'wheat', 'palladium', 'cocoa', 'oat', 'sunseed', 'lead', 'coconut', 'hog', 'sorghum', 'money-supply', 'propane', 'coffee', 'carcass', 'dfl', 'corn', 'sun-oil', 'veg-oil', 'lei', 'castor-oil', 'nat-gas', 'alum', 'yen', 'rand', 'unknown', 'ship', 'zinc', 'jobs', 'rye', 'nickel', 'soy-meal', 'pet-chem', 'iron-steel', 'acq', 'orange', 'groundnut-oil', 'barley', 'crude', 'reserves', 'instal-debt']\n"
     ]
    }
   ],
   "source": [
    "# this gives us all the groups (from training subfolder, but same for test)\n",
    "groups = [g for g in os.listdir(os.path.join(data_dir, \"training\")) if os.path.isdir(os.path.join(data_dir, \"training\", g))]\n",
    "print (groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all of our documents (news articles) into a single list called `docs`. We'll iterate through each group, grab all of the posts in each group (from both training and test directories), and add the text of the post into the `docs` list. We will make sure to exclude duplicate posts by cheking if we've seen the post index before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading group 1 / 91\n",
      "reading group 11 / 91\n",
      "reading group 21 / 91\n",
      "reading group 31 / 91\n",
      "reading group 41 / 91\n",
      "reading group 51 / 91\n",
      "reading group 61 / 91\n",
      "reading group 71 / 91\n",
      "reading group 81 / 91\n",
      "reading group 91 / 91\n",
      "\n",
      "we have 12897 documents in 91 groups\n",
      "\n",
      "here is document 100:\n",
      "\n",
      "\n",
      "RPT - ARGENTINE GRAIN/OILSEED EXPORT PRICES ADJUSTED\n",
      "\n",
      "    BUENOS AIRES, April 8 - The Argentine Grain Board adjusted\n",
      "minimum export prices of grain and oilseed products in dlrs per\n",
      "tonne FOB, previous in brackets, as follows:\n",
      "    Sorghum 64 (63), sunflowerseed cake and expellers 103 (102)\n",
      ", pellets 101 (100), meal 99 (98), linseed oil 274 (264),\n",
      "groundnutseed oil 450 (445), soybean oil 300 (290), rapeseed\n",
      "oil 290 (280).\n",
      "    Sunflowerseed oil for shipment through May 323 (313) and\n",
      "june onwards 330 (320).\n",
      "    The board also adjusted export prices at which export taxes\n",
      "are levied in dlrs per tonne FOB, previous in brackets, as\n",
      "follows:\n",
      "    Bran pollard wheat 40 (42), pellets 42 (44).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "docs = []\n",
    "post_idx = []\n",
    "for g, group in enumerate(groups):\n",
    "    if g%10==0:\n",
    "        print (\"reading group %d / %d\"%(g+1, len(groups)))\n",
    "    posts_training = [os.path.join(data_dir, \"training\", group, p) for p in os.listdir(os.path.join(data_dir, \"training\", group)) if os.path.isfile(os.path.join(data_dir, \"training\", group, p))]\n",
    "    posts_test = [os.path.join(data_dir, \"test\", group, p) for p in os.listdir(os.path.join(data_dir, \"test\", group)) if os.path.isfile(os.path.join(data_dir, \"test\", group, p))]\n",
    "    posts = posts_training + posts_test\n",
    "    for post in posts:\n",
    "        idx = post.split(\"/\")[-1]\n",
    "        if idx not in post_idx:\n",
    "            post_idx.append(idx)\n",
    "            with open(post, \"r\", errors=\"replace\") as p:\n",
    "                p\n",
    "                raw_text = p.read()\n",
    "                raw_text = re.sub(r'[^\\x00-\\x7f]',r'', raw_text) \n",
    "                docs.append(raw_text)\n",
    "\n",
    "print(\"\\nwe have %d documents in %d groups\"%(len(docs), len(groups)))\n",
    "print(\"\\nhere is document 100:\\n%s\"%docs[100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use `sklearn`'s `TfidfVectorizer` to compute the tf-idf matrix of our collection of documents. The tf-idf matrix is an `n`x`m` matrix with the `n` rows corresponding to our `n` documents and the `m` columns corresponding to our terms. The values corresponds to the \"importance\" of each term to each document, where importance is *. In this case, terms are just all the unique words in the corpus, minus english _stopwords_, which are the most common words in the english language, e.g. \"it\", \"they\", \"and\", \"a\", etc. In some cases, terms can be n-grams (n-length sequences of words) or more complex, but usually just words.\n",
    "\n",
    "To compute our tf-idf matrix, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eb65634a726e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(docs)\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variable `tfidf` is a sparse matrix with a row for each document, and a column for each unique term in the corpus. \n",
    "\n",
    "Thus, we can interpret each row of this matrix as a feature vector which describes a document. Two documents which have identical rows have the same collection of words in them, although not necessarily in the same order; word order is not preserved in the tf-idf matrix. Regardless, it seems reasonable to expect that if two documents have similar or close tf-idf vectors, they probably have similar content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5599d9d4d238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mall_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "doc_idx = 5\n",
    "\n",
    "doc_tfidf = tfidf.getrow(doc_idx)\n",
    "all_terms = vectorizer.get_feature_names()\n",
    "terms = [all_terms[i] for i in doc_tfidf.indices]\n",
    "values = doc_tfidf.data\n",
    "\n",
    "print(docs[doc_idx])\n",
    "print(\"document's term-frequency pairs:\")\n",
    "print(\", \".join(\"\\\"%s\\\"=%0.2f\"%(t,v) for t,v in zip(terms,values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice however, the term-document matrix alone has several disadvantages. For one, it is very high-dimensional and sparse (mostly zeroes), thus it is computationally costly. \n",
    "\n",
    "Additionally, it ignores similarity among groups of terms. For example, the words \"seat\" and \"chair\" are related, but in a raw term-document matrix they are separate columns. So two sentences with one of each word will not be computed as similarly.\n",
    "\n",
    "One solution is to use latent semantic analysis (LSA, or sometimes called latent semantic indexing). LSA is a dimensionality reduction technique closely related to principal component analysis, which is commonly used to reduce a high-dimensional set of terms into a lower-dimensional set of \"concepts\" or components which are linear combinations of the terms.\n",
    "\n",
    "To do so, we use `sklearn`'s `TruncatedSVD` function which gives us the LSA by computing a [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) of the tf-idf matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-605ae8203b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfidf_lsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components=100)\n",
    "tfidf_lsa = lsa.fit_transform(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret this? `lsa` holds our latent semantic analysis, expressing our 100 concepts. It has a vector for each concept, which holds the weight of each term to that concept. `tfidf_lsa` is our transformed document matrix where each document is a weighted sum of the concepts. \n",
    "\n",
    "In a simpler analysis with, for example, two topics (sports and tacos), one concept might assign high weights for sports-related terms (ball, score, tournament) and the other one might have high weights for taco-related concepts (cheese, tomato, lettuce). In a more complex one like this one, the concepts may not be as interpretable. Nevertheless, we can investigate the weights for each concept, and look at the top-weighted ones. For example, here are the top terms in concept 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-11be9b75b5c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0midx_top_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lsa' is not defined"
     ]
    }
   ],
   "source": [
    "components = lsa.components_[1]\n",
    "all_terms = vectorizer.get_feature_names()\n",
    "\n",
    "idx_top_terms = sorted(range(len(components)), key=lambda k: components[k])\n",
    "\n",
    "print(\"10 highest-weighted terms in concept 1:\")\n",
    "for t in idx_top_terms[:10]:\n",
    "    print(\" - %s : %0.02f\"%(all_terms[t], t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top terms in concept 1 appear related to accounting balance sheets; terms like \"net\", \"loss\", \"profit\".\n",
    "\n",
    "Now, back to our documents. Recall that `tfidf_lsa` is a transformation of our original tf-idf matrix from the term-space into a concept-space. The concept space is much more valuable, and we can use it to query most similar documents. We expect that two documents which about similar things should have similar vectors in `tfidf_lsa`. We can use a simple distance metric to measure the similarity, euclidean distance or cosine similarity being the two most common. \n",
    "\n",
    "Here, we'll select a single query document (index 300), calculate the distance of every other document to our query document, and take the one with the smallest distance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-72775d1127c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# take the concept representation of our query document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "query_idx = 400\n",
    "\n",
    "# take the concept representation of our query document\n",
    "query_features = tfidf_lsa[query_idx]\n",
    "\n",
    "# calculate the distance between query and every other document\n",
    "distances = [ distance.euclidean(query_features, feat) for feat in tfidf_lsa ]\n",
    "    \n",
    "# sort indices by distances, excluding the first one which is distance from query to itself (0)\n",
    "idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])[1:]\n",
    "\n",
    "# print our results\n",
    "query_doc = docs[query_idx]\n",
    "return_doc = docs[idx_closest[0]]\n",
    "print(\"QUERY DOCUMENT:\\n %s \\nMOST SIMILAR DOCUMENT TO QUERY:\\n %s\" %(query_doc, return_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting find! Our query document appears to be about tax incentives for domestic oil and natural gas. Our return document is a related article about the same topic. Try looking at the next few closest results. A quick inspection reveals that most of them are about the same story.\n",
    "\n",
    "Thus we see the value of this procedure. It gives us a way to quickly identify articles which are related to each other. This can greatly aide journalists who have to sift through a lot of content which is not always indexed or organized usefully. \n",
    "\n",
    "More creatively, we can think of other ways this can be made useful. For example, what if instead of making our documents the articles themselves, what if they were made to be paragraphs from the articles? Then, we could potentially discover relevant paragraphs about one topic which are buried in an article which is otherwise about a different topic. We can combine this with handcrafted filters as well (date range, presence of a word or name, etc); perhaps you want to quickly find every quote politican X has made about topic Y. This provides an effective means to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
